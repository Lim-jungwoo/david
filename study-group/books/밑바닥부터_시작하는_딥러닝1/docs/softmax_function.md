# 소프트맥스(Softmax) 함수 정리

## 📌 정의

소프트맥스 함수는 여러 개의 실수 입력값을 받아서,  
**각 항목이 정답일 확률**처럼 해석할 수 있는 **0~1 사이의 값**으로 변환하는 함수입니다.

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- 입력값: $$( z_i $$) (logit, 신경망의 마지막 선형 변환 결과)  
- 클래스 개수: \( K \)  

**Softmax 정의:**

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
$$

**출력 범위:**

$$
0 \leq \text{softmax}(z_i) \leq 1
$$

**합의 성질:**

$$
\sum_{i=1}^K \text{softmax}(z_i) = 1
$$


---

## 📌 특징

1. **확률 분포 생성**
   - 모든 출력값의 합이 1이 되어 확률로 해석 가능
2. **값 차이 강조**
   - 큰 값은 더 큰 확률, 작은 값은 더 작은 확률로 변환
3. **지수 함수 사용**
   - 모든 값을 양수로 변환
   - 상대적 크기를 보존하며 스케일링

---

## 📌 예시

입력:

$$
z = [2.0, 1.0, 0.1]
$$

계산:

$$
\text{softmax}(z) \approx [0.659, 0.242, 0.099]
$$

해석:

- 첫 번째 클래스: 65.9% 확률
- 두 번째 클래스: 24.2% 확률
- 세 번째 클래스: 9.9% 확률

---

## 📌 활용

- **다중 분류(Multi-class Classification)** 문제의 출력층
  - 예: 손글씨 숫자 인식(0~9), 이미지 분류(고양이/개/자동차 등)
- 출력값을 **확률 분포**로 해석할 때 필수

---

⚡ **한 줄 요약**  
소프트맥스 함수는 입력값들을 **확률 분포(합=1)**로 변환하여,  
신경망의 **다중 분류 문제 출력층**에서 사용된다.
